{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bcharyyev/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcharyyev/.local/lib/python3.5/site-packages/ipykernel_launcher.py:92: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bcharyyev/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcharyyev/.local/lib/python3.5/site-packages/ipykernel_launcher.py:117: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.models import load_model, Sequential, Model\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Dropout, Input, MaxPooling1D,Conv1D,LSTM, Flatten, Reshape\n",
    "from keras.layers.merge import Add, Multiply, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "    \n",
    "class Actor_Critic:\n",
    "    def __init__(self, nx, ny, s_link, sess, batch):\n",
    "        self.nx = nx[0]\n",
    "        self.nx_lidar = 10\n",
    "        self.nx_obs = 14\n",
    "        self.ny = ny[0]\n",
    "        self.lr_actor = 1e-4\n",
    "        self.lr_critic = 3e-4\n",
    "        self.batch = batch\n",
    "        self.gamma = 0.99\n",
    "        self.alpha = 0.1\n",
    "        self.s_link =s_link \n",
    "        self.sess = sess\n",
    "        self.deck = deque(maxlen=4000)\n",
    "        self.e = 1.0\n",
    "        self.e_= 0.01\n",
    "        self.dc= 0.9999\n",
    "        self.tau = 1e-3\n",
    "        self.weight_decay = 0.0001\n",
    "        self.los = []\n",
    "        self.parameters={'lr_actor': self.lr_actor,'lr_critic':self.lr_critic,'gamma':self.gamma,\n",
    "                         'alpha':self.alpha, 'tau':self.tau,'dc':self.dc,'Batch':self.batch}\n",
    "        \n",
    "        \n",
    "\n",
    "        #Actor-Critic\n",
    "        self.actor_lidar_input, self.actor_state_input, self.actor_local = self.Actor()      \n",
    "        _,_, self.actor_target = self.Actor()\n",
    "        self.actor_critic_grads = tf.placeholder(tf.float32, [None,self.ny])\n",
    "        actor_local_weights = self.actor_local.trainable_weights\n",
    "        self.actor_grads = tf.gradients(self.actor_local.output, actor_local_weights, -self.actor_critic_grads)\n",
    "        grads = zip(self.actor_grads, actor_local_weights)\n",
    "        self.optimize = tf.train.AdamOptimizer(self.lr_actor).apply_gradients(grads)\n",
    "  \n",
    "\n",
    "        self.critic_lidar_input, self.critic_state_input, self.critic_action_input, self.critic_local = self.Critic()      \n",
    "        _,_, _, self.critic_target = self.Critic()\n",
    "        self.critic_grads = tf.gradients(self.critic_local.output,  self.critic_action_input)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.ep_rewards=[]\n",
    "        \n",
    "    def choose_action(self,observation):\n",
    "        if np.random.rand() <= self.e : \n",
    "            action = np.random.uniform(-1,1,4)\n",
    "            return action       \n",
    "        state = observation[0][:14].reshape((1,14))\n",
    "        lidar = observation[0][14:].reshape((1,10,1))\n",
    "        action = self.actor_local.predict([lidar,state])\n",
    "        return action\n",
    "                    \n",
    "    def storing(self, observation, action, reward, observation_new, flags ):\n",
    "        self.deck.append((observation, action, reward, observation_new, flags ))\n",
    "        self.ep_rewards.append(reward)\n",
    "        \n",
    "    def save(self,name):\n",
    "        self.actor_local.save(name)\n",
    "        self.critic_local.save(name)\n",
    "\n",
    "    def Actor(self):                     \n",
    "        lidar_input = Input(shape=(self.nx_lidar,1))\n",
    "        lidar_conv = Conv1D(64, 4, activation='relu')(lidar_input)\n",
    "        pool = MaxPooling1D(4)(lidar_conv)\n",
    "        flat = Flatten()(pool)\n",
    "               \n",
    "        state_input = Input(shape=(self.nx_obs,))\n",
    "        state_h1 = Dense(192, activation='relu')(state_input)\n",
    "        \n",
    "        merged = Concatenate()([flat,state_h1])\n",
    "        merged_reshaped = Reshape((256,1))(merged)\n",
    "        merged_lstm = LSTM(256,activation='relu',input_shape=(1,256,1))(merged_reshaped)\n",
    "        output = Dense(self.ny, activation='tanh')(merged_lstm)\n",
    "        \n",
    "        model = Model(input=[lidar_input,state_input], output=output)\n",
    "        adam = Adam(lr=self.lr_actor)\n",
    "        model.compile(loss='mse', optimizer=adam)\n",
    "        return lidar_input,state_input, model\n",
    "\n",
    "    def Critic(self):                     \n",
    "        lidar_input = Input(shape=(self.nx_lidar,1))\n",
    "        lidar_conv = Conv1D(64, 4, activation='relu',input_shape=(self.nx_lidar,1))(lidar_input)\n",
    "        pool = MaxPooling1D(4)(lidar_conv)\n",
    "        flat= Flatten()(pool)\n",
    "        \n",
    "        state_input = Input(shape=(self.nx_obs,))\n",
    "        state_h1 = Dense(192, activation='relu')(state_input)\n",
    "        \n",
    "        action_input = Input(shape=(self.ny,))\n",
    "        action_h1    = Dense(64, activation='relu')(action_input)\n",
    "        \n",
    "        merge1 = Concatenate()([flat,state_h1])\n",
    "        merged_dense = Dense(256, activation='relu')(merge1)\n",
    "        \n",
    "        merge2 = Concatenate()([merged_dense,action_h1])\n",
    "        merge2reshaped = Reshape((320,1))(merge2)\n",
    "        merge_lstm = LSTM(320, activation='relu',input_shape=(1,320,1))(merge2reshaped)\n",
    "        output= Dense(1,activation='linear')(merge_lstm)\n",
    "        \n",
    "        model  = Model(input=[lidar_input,state_input,action_input], output=output)\n",
    "        adam  = Adam(lr=self.lr_critic)\n",
    "        model.compile(loss=\"mse\", optimizer=adam)\n",
    "        return lidar_input,state_input, action_input, model\n",
    "    \n",
    "\n",
    "    def _train_critic(self, sample_indx):\n",
    "        for observation, act, reward, obs_new, done in sample_indx:  \n",
    "            Q_target = np.array(reward).reshape(1,-1)\n",
    "            act = act.reshape(1,-1)\n",
    "            state = observation[0][:14].reshape((1,14))\n",
    "            lidar = observation[0][14:].reshape((1,10,1))\n",
    "            state_new = obs_new[0][:14].reshape((1,14))\n",
    "            lidar_new = obs_new[0][14:].reshape((1,10,1))\n",
    "            if not done:\n",
    "                target_action = self.actor_target.predict([lidar_new,state_new])\n",
    "                future_reward = self.critic_target.predict([lidar_new,state_new, target_action])[0][0]\n",
    "                Q_target =(1-self.alpha)*Q_target +  self.alpha* self.gamma * future_reward\n",
    "                Q_target = Q_target.reshape(1,-1)\n",
    "            self.critic_local.fit(x=[lidar,state,act],y=Q_target, verbose=0, epochs=1)   \n",
    "            \n",
    "            \n",
    "    def _train_actor(self, sample_indx):\n",
    "        for observation, act, reward, observation_new, _ in sample_indx:\n",
    "            state = observation[0][:14].reshape((1,14))\n",
    "            lidar = observation[0][14:].reshape((1,10,1))\n",
    "\n",
    "            predicted_action = self.actor_local.predict([lidar,state])\n",
    "            grads = self.sess.run(self.critic_grads, feed_dict = {\n",
    "                    self.critic_lidar_input : lidar,\n",
    "                    self.critic_state_input: state,\n",
    "                    self.critic_action_input: predicted_action})[0]\n",
    "            \n",
    "            self.sess.run(self.optimize, feed_dict={\n",
    "                    self.actor_lidar_input: lidar,\n",
    "                    self.actor_state_input: state,\n",
    "                    self.actor_critic_grads: grads})            \n",
    "            \n",
    "    def _update_actor_target(self):\n",
    "        actor_local_weights  = self.actor_local.get_weights()\n",
    "        actor_target_weights =self.actor_target.get_weights()\n",
    "        \n",
    "        for i in range(len(actor_target_weights)):\n",
    "            actor_target_weights[i] = self.tau*actor_local_weights[i] + (1-self.tau)*actor_target_weights[i]\n",
    "        self.actor_target.set_weights(actor_target_weights)          \n",
    "            \n",
    "    def _update_critic_target(self):\n",
    "        critic_local_weights  = self.critic_local.get_weights()\n",
    "        critic_target_weights = self.critic_target.get_weights()\n",
    "\n",
    "        for i in range(len(critic_target_weights)):\n",
    "            critic_target_weights[i] = self.tau*critic_local_weights[i] + (1-self.tau)*critic_target_weights[i]\n",
    "        self.critic_target.set_weights(critic_target_weights)\t\t\n",
    "\n",
    "    def update_target(self):\n",
    "        self._update_actor_target()\n",
    "        self._update_critic_target()\n",
    "            \n",
    "      \n",
    "    def TRAIN(self, batch):\n",
    "\n",
    "        if len(self.deck) < batch:\n",
    "            return\n",
    "\n",
    "        sample_indx = random.sample(self.deck, batch)\n",
    "        time_all = {}\n",
    "\n",
    "        self._train_critic(sample_indx)\n",
    "        self._train_actor(sample_indx)\n",
    "        self.update_target()\n",
    "        self.ep_rewards= []\n",
    "                     \n",
    "        if self.e >= self.e_:\n",
    "            self.e *= self.dc\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    env = gym.make('BipedalWalker-v2')\n",
    "    env = env.unwrapped\n",
    "    nx = env.observation_space.shape  \n",
    "    ny = env.action_space.shape \n",
    "    sess = tf.Session()\n",
    "    K.set_session(sess)\n",
    "    agent = Actor_Critic(nx,ny, \"BipedalWalker_model.h5\", sess, 15)\n",
    "        \n",
    "    for i in range(3): \n",
    "        observation = env.reset()         \n",
    "        observation = observation.reshape(1,-1)                \n",
    "        counter=0\n",
    "        while counter<10000:            \n",
    "            counter=counter+1\n",
    "            env.render()\n",
    "            action = agent.choose_action(observation)\n",
    "            action = action.reshape((4,))\n",
    "            observation_new, reward, flag, inf = env.step(np.clip(action,-1,1))\n",
    "            observation_new = observation_new.reshape((1,24))                    \n",
    "            agent.storing(observation, action, reward, observation_new, flag)   \n",
    "            observation = observation_new         \n",
    "\n",
    "    \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
